{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Alpamayo-R1 Interactive Inference (Local/MPS)\n\nInteractive notebook for exploring model inference on driving scenes using Apple Silicon MPS backend.\n\n**Requirements:**\n- Apple Silicon Mac (M1/M2/M3) or CPU fallback\n- `pip install -e tools/alpamayo/`\n\n**Note:** Uses 'eager' attention instead of flash-attn. Inference will be slower than on NVIDIA GPUs."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Paths\n",
    "PROJECT_ROOT = Path.cwd().parent.parent\n",
    "CLIP_IDS_FILE = PROJECT_ROOT / \"tools\" / \"alpamayo\" / \"notebooks\" / \"clip_ids.parquet\"\n",
    "\n",
    "# Device detection\n",
    "if torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "    DEVICE = \"mps\"\n",
    "    DEVICE_NAME = \"Apple MPS\"\n",
    "else:\n",
    "    DEVICE = \"cpu\"\n",
    "    DEVICE_NAME = \"CPU\"\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"Device: {DEVICE_NAME}\")\n",
    "print(f\"MPS available: {torch.backends.mps.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1. Load Model\n\nUsing 'eager' attention implementation (flash-attn not available on Mac)."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from alpamayo_r1.models.alpamayo_r1 import AlpamayoR1\nfrom alpamayo_r1 import load_physical_aiavdataset\nfrom alpamayo_r1 import helper\n\nMODEL_ID = \"nvidia/Alpamayo-R1-10B\"\n\n# Use float32 for MPS compatibility (bfloat16 has limited support)\nMODEL_DTYPE = torch.float32\n\nprint(f\"Loading {MODEL_ID}...\")\nprint(f\"  dtype: {MODEL_DTYPE}\")\nprint(f\"  attention: eager\")\n\nmodel = AlpamayoR1.from_pretrained(\n    MODEL_ID, \n    dtype=MODEL_DTYPE,\n    attn_implementation=\"eager\",\n).to(DEVICE)\n\nprocessor = helper.get_processor(model.tokenizer)\nprint(\"Model loaded!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Scene Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load available clip IDs\n",
    "clip_ids_df = pd.read_parquet(CLIP_IDS_FILE)\n",
    "all_clip_ids = clip_ids_df[\"clip_id\"].tolist()\n",
    "print(f\"Available clips: {len(all_clip_ids)}\")\n",
    "\n",
    "# Select a clip (change index or use random)\n",
    "CLIP_INDEX = 0  # Change this to explore different clips\n",
    "# import random; CLIP_INDEX = random.randint(0, len(all_clip_ids) - 1)  # Uncomment for random\n",
    "\n",
    "clip_id = all_clip_ids[CLIP_INDEX]\n",
    "t0_us = 5_000_000  # 5 seconds into clip\n",
    "\n",
    "print(f\"\\nLoading clip: {clip_id}\")\n",
    "print(f\"Timestamp: {t0_us / 1e6:.1f}s\")\n",
    "\n",
    "data = load_physical_aiavdataset(clip_id, t0_us=t0_us)\n",
    "print(f\"Image frames shape: {data['image_frames'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualize Scene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_camera_views(image_frames, frame_idx=-1):\n",
    "    \"\"\"Display all 4 camera views.\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 9))\n",
    "    \n",
    "    camera_names = [\n",
    "        \"Cross-Left 120\\u00b0\",\n",
    "        \"Front-Wide 120\\u00b0\",\n",
    "        \"Cross-Right 120\\u00b0\",\n",
    "        \"Front-Tele 30\\u00b0\"\n",
    "    ]\n",
    "    \n",
    "    for idx, (ax, name) in enumerate(zip(axes.flat, camera_names)):\n",
    "        img = image_frames[idx, frame_idx].permute(1, 2, 0).numpy().astype(np.uint8)\n",
    "        ax.imshow(img)\n",
    "        ax.set_title(name, fontsize=12)\n",
    "        ax.axis('off')\n",
    "    \n",
    "    plt.suptitle(f\"Clip: {clip_id[:16]}... | t0 = {t0_us/1e6:.1f}s\", fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "show_camera_views(data['image_frames'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare inputs\n",
    "messages = helper.create_message(data[\"image_frames\"].flatten(0, 1))\n",
    "inputs = processor.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=False,\n",
    "    continue_final_message=True,\n",
    "    return_dict=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "\n",
    "model_inputs = {\n",
    "    \"tokenized_data\": inputs,\n",
    "    \"ego_history_xyz\": data[\"ego_history_xyz\"],\n",
    "    \"ego_history_rot\": data[\"ego_history_rot\"],\n",
    "}\n",
    "model_inputs = helper.to_device(model_inputs, DEVICE)\n",
    "\n",
    "# Run inference (no autocast for MPS/CPU)\n",
    "print(\"Running inference...\")\n",
    "if DEVICE == \"mps\":\n",
    "    torch.mps.manual_seed(42)\n",
    "\n",
    "pred_xyz, pred_rot, extra = model.sample_trajectories_from_data_with_vlm_rollout(\n",
    "    data=model_inputs,\n",
    "    top_p=0.98,\n",
    "    temperature=0.6,\n",
    "    num_traj_samples=1,\n",
    "    max_generation_length=256,\n",
    "    return_extra=True,\n",
    ")\n",
    "\n",
    "print(\"Inference complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Chain-of-Causation Reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coc_text = extra[\"cot\"][0] if extra.get(\"cot\") else \"No CoC generated\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"CHAIN-OF-CAUSATION REASONING\")\n",
    "print(\"=\" * 80)\n",
    "print(coc_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Trajectory Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract trajectories\n",
    "gt_xyz = data[\"ego_future_xyz\"].cpu().numpy()[0, 0]  # (64, 3)\n",
    "pred_xyz_np = pred_xyz.cpu().numpy()[0, 0, 0]  # (64, 3)\n",
    "history_xyz = data[\"ego_history_xyz\"].cpu().numpy()[0, 0]  # (16, 3)\n",
    "\n",
    "# Compute minADE\n",
    "diff = np.linalg.norm(pred_xyz_np[:, :2] - gt_xyz[:, :2], axis=1)\n",
    "min_ade = diff.mean()\n",
    "\n",
    "print(f\"minADE: {min_ade:.3f} meters\")\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "# History\n",
    "ax.plot(history_xyz[:, 0], history_xyz[:, 1], 'b.-', \n",
    "        label=f'History (1.6s)', linewidth=2, markersize=6)\n",
    "\n",
    "# Ground truth future\n",
    "ax.plot(gt_xyz[:, 0], gt_xyz[:, 1], 'g.-', \n",
    "        label=f'Ground Truth (6.4s)', linewidth=2, markersize=4)\n",
    "\n",
    "# Predicted future\n",
    "ax.plot(pred_xyz_np[:, 0], pred_xyz_np[:, 1], 'r.-', \n",
    "        label=f'Predicted (minADE={min_ade:.2f}m)', linewidth=2, markersize=4)\n",
    "\n",
    "# Ego position at t0\n",
    "ax.scatter([0], [0], c='black', s=200, marker='*', label='t0 (ego)', zorder=5)\n",
    "\n",
    "ax.set_xlabel('X (meters)')\n",
    "ax.set_ylabel('Y (meters)')\n",
    "ax.set_title(f'Trajectory Comparison\\nClip: {clip_id[:16]}...')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_aspect('equal')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Quick Test Function\n",
    "\n",
    "Convenience function to test different clips quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_clip(clip_id: str, t0_us: int = 5_000_000, show_images: bool = True):\n",
    "    \"\"\"Run full inference pipeline on a clip.\"\"\"\n",
    "    \n",
    "    # Load data\n",
    "    data = load_physical_aiavdataset(clip_id, t0_us=t0_us)\n",
    "    \n",
    "    if show_images:\n",
    "        show_camera_views(data['image_frames'])\n",
    "    \n",
    "    # Prepare inputs\n",
    "    messages = helper.create_message(data[\"image_frames\"].flatten(0, 1))\n",
    "    inputs = processor.apply_chat_template(\n",
    "        messages, tokenize=True, add_generation_prompt=False,\n",
    "        continue_final_message=True, return_dict=True, return_tensors=\"pt\",\n",
    "    )\n",
    "    model_inputs = helper.to_device({\n",
    "        \"tokenized_data\": inputs,\n",
    "        \"ego_history_xyz\": data[\"ego_history_xyz\"],\n",
    "        \"ego_history_rot\": data[\"ego_history_rot\"],\n",
    "    }, DEVICE)\n",
    "    \n",
    "    # Inference (no autocast for MPS/CPU)\n",
    "    pred_xyz, pred_rot, extra = model.sample_trajectories_from_data_with_vlm_rollout(\n",
    "        data=model_inputs, top_p=0.98, temperature=0.6,\n",
    "        num_traj_samples=1, max_generation_length=256, return_extra=True,\n",
    "    )\n",
    "    \n",
    "    # Results\n",
    "    coc = extra[\"cot\"][0] if extra.get(\"cot\") else \"\"\n",
    "    gt_xy = data[\"ego_future_xyz\"].cpu().numpy()[0, 0, :, :2]\n",
    "    pred_xy = pred_xyz.cpu().numpy()[0, 0, 0, :, :2]\n",
    "    min_ade = np.linalg.norm(pred_xy - gt_xy, axis=1).mean()\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Clip: {clip_id}\")\n",
    "    print(f\"minADE: {min_ade:.3f} m\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"CoC: {coc}\")\n",
    "    \n",
    "    return {\"clip_id\": clip_id, \"coc\": coc, \"min_ade\": min_ade, \"data\": data, \"pred_xyz\": pred_xyz}\n",
    "\n",
    "# Example: test a random clip\n",
    "# import random\n",
    "# result = test_clip(random.choice(all_clip_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Batch Test (Optional)\n",
    "\n",
    "Test multiple clips and collect statistics.\n",
    "\n",
    "**Note:** This will be slow on MPS/CPU compared to NVIDIA GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to run batch test\n",
    "# import random\n",
    "# N_CLIPS = 5  # Keep small for MPS/CPU\n",
    "# test_clips = random.sample(all_clip_ids, N_CLIPS)\n",
    "# \n",
    "# results = []\n",
    "# for i, cid in enumerate(test_clips):\n",
    "#     print(f\"\\n[{i+1}/{N_CLIPS}] Testing {cid[:16]}...\")\n",
    "#     try:\n",
    "#         r = test_clip(cid, show_images=False)\n",
    "#         results.append(r)\n",
    "#     except Exception as e:\n",
    "#         print(f\"  Error: {e}\")\n",
    "# \n",
    "# ades = [r[\"min_ade\"] for r in results]\n",
    "# print(f\"\\n\\nBatch Results ({len(results)} clips):\")\n",
    "# print(f\"  minADE: {np.mean(ades):.3f} +/- {np.std(ades):.3f} m\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
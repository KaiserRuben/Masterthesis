{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Threshold Optimization on Real VLM Data\n",
    "\n",
    "**Goal:** Find optimal IoU thresholds using real Qwen3-VL predictions under perturbation.\n",
    "\n",
    "**Approach:** Information-theoretic optimization\n",
    "$$t^* = \\arg\\max_t I(C_t(f(x)); S(x))$$\n",
    "\n",
    "Where:\n",
    "- $C_t$ = behavioral classes using thresholds $t$\n",
    "- $S(x)$ = signal (perturbation magnitude OR embedding distance)\n",
    "\n",
    "**Data:** RefCOCO small subset (51 samples \u00d7 25 perturbations = 1,275 data points)\n",
    "\n",
    "**Expected runtime:** ~60 minutes (Qwen3-VL 8b)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-11T14:47:20.650163Z",
     "start_time": "2026-01-11T14:47:17.231933Z"
    }
   },
   "source": [
    "# Fix imports after reorganization\n",
    "import sys\n",
    "sys.path.insert(0, 'scripts')\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import optimize\n",
    "from sklearn.metrics import mutual_info_score\n",
    "from PIL import Image, ImageEnhance, ImageFilter\n",
    "import ollama_proxy as ollama\n",
    "import io\n",
    "import base64\n",
    "import re\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "\n",
    "from refcoco_loader import load_refcoco, get_sample_info\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "np.random.seed(42)"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load RefCOCO Small Subset\n",
    "\n",
    "Load the 51 samples saved from notebook 01."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-11T14:47:23.251377Z",
     "start_time": "2026-01-11T14:47:20.651202Z"
    }
   },
   "source": [
    "# Load small subset indices\n",
    "small_indices = np.load('small_subset_indices.npy')\n",
    "print(f\"Loading {len(small_indices)} samples from RefCOCO...\")\n",
    "\n",
    "# Load full validation set\n",
    "dataset = load_refcoco('val')\n",
    "\n",
    "# Extract subset\n",
    "samples = [dataset[int(idx)] for idx in small_indices]\n",
    "\n",
    "print(f\"\\nLoaded {len(samples)} samples for threshold optimization\")\n",
    "print(f\"First sample:\")\n",
    "info = get_sample_info(samples[0])\n",
    "print(f\"  Image size: {info['image_size']}\")\n",
    "print(f\"  Expressions: {info['expressions'][:2]}\")\n",
    "print(f\"  Bbox: {info['bbox_normalized']}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 51 samples from RefCOCO...\n",
      "\n",
      "Loaded 51 samples for threshold optimization\n",
      "First sample:\n",
      "  Image size: (299, 409)\n",
      "  Expressions: ['guy in plaid', 'mr plaid']\n",
      "  Bbox: [0.00307692 0.02246944 0.38117058 0.43819071]\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. VLM Setup\n",
    "\n",
    "Configure Qwen3-VL via Ollama for bbox prediction and embedding extraction."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-11T14:47:23.322941Z",
     "start_time": "2026-01-11T14:47:23.301772Z"
    }
   },
   "source": "class VLMPredictor:\n    \"\"\"\n    Wrapper for Qwen3-VL bbox prediction.\n    \n    See QWEN3VL_GROUNDING.md for format details.\n    \"\"\"\n    def __init__(self, model_name=\"qwen3-vl:8b\", embed_model=\"qwen3-embedding:latest\"):\n        self.model_name = model_name\n        self.embed_model = embed_model\n        self.parse_failures = 0\n        self.total_calls = 0\n        print(f\"Initialized VLM: {model_name}\")\n        print(f\"Embedding model: {embed_model}\")\n    \n    def image_to_base64(self, image: Image.Image) -> str:\n        \"\"\"Convert PIL image to base64.\"\"\"\n        # Ensure RGB mode (Qwen3-VL requires RGB)\n        if image.mode != 'RGB':\n            image = image.convert('RGB')\n        \n        buffered = io.BytesIO()\n        image.save(buffered, format=\"PNG\")\n        return base64.b64encode(buffered.getvalue()).decode()\n    \n    def predict_bbox(self, image: Image.Image, expression: str) -> np.ndarray:\n        \"\"\"\n        Predict bbox for referring expression.\n        \n        Returns: [x1, y1, x2, y2] normalized [0, 1]\n        \"\"\"\n        self.total_calls += 1\n        \n        # Qwen3-VL uses [0, 1000] coordinate system\n        prompt = f'Where is \"{expression}\" in this image? Output the bounding box in format: {{\"bbox_2d\": [x_min, y_min, x_max, y_max]}} using coordinates 0-1000.'\n        \n        img_b64 = self.image_to_base64(image)\n        \n        try:\n            response = ollama.chat(\n                model=self.model_name,\n                messages=[{\n                    'role': 'user',\n                    'content': prompt,\n                    'images': [img_b64]\n                }]\n            )\n            \n            content = response['message']['content']\n            bbox = self.parse_bbox(content)\n            \n            return bbox\n            \n        except Exception as e:\n            print(f\"Error predicting bbox: {e}\")\n            self.parse_failures += 1\n            return np.array([0.0, 0.0, 0.0, 0.0])  # Zero-IoU default\n    \n    def parse_bbox(self, text: str) -> np.ndarray:\n        \"\"\"\n        Extract bbox from model response and convert to [0,1].\n        \n        Qwen3-VL returns coordinates in [0, 1000] range.\n        \"\"\"\n        # Try to extract JSON format first\n        json_match = re.search(r'\\{\"bbox_2d\"\\s*:\\s*\\[([^\\]]+)\\]\\}', text)\n        if json_match:\n            coords_str = json_match.group(1)\n            numbers = re.findall(r'[-+]?[0-9]*\\.?[0-9]+', coords_str)\n        else:\n            # Fallback: extract any numbers\n            numbers = re.findall(r'[-+]?[0-9]*\\.?[0-9]+', text)\n        \n        if len(numbers) >= 4:\n            # Parse from [0, 1000] range\n            bbox_1000 = np.array([float(numbers[i]) for i in range(4)])\n            \n            # Convert to [0, 1]\n            bbox = bbox_1000 / 1000.0\n            bbox = np.clip(bbox, 0, 1)\n            \n            # Validate\n            if bbox[0] >= bbox[2] or bbox[1] >= bbox[3]:\n                self.parse_failures += 1\n                if self.parse_failures <= 3:  # Only print first 3\n                    print(f\"Warning: Invalid bbox {bbox} from: '{text[:100]}'\")\n                return np.array([0.0, 0.0, 0.0, 0.0])  # Zero-area box = IoU 0\n            \n            return bbox\n        else:\n            self.parse_failures += 1\n            if self.parse_failures <= 3:  # Only print first 3\n                print(f\"Warning: Could not parse bbox. Response length: {len(text)} chars\")\n                print(f\"  First 200 chars: '{text[:200]}'\")\n            return np.array([0.0, 0.0, 0.0, 0.0])  # Zero-area box = IoU 0\n    \n    def get_embedding(self, text: str) -> np.ndarray:\n        \"\"\"Get text embedding.\"\"\"\n        try:\n            response = ollama.embeddings(model=self.embed_model, prompt=text)\n            return np.array(response['embedding'])\n        except Exception as e:\n            print(f\"Error getting embedding: {e}\")\n            return np.zeros(4096)  # Fallback\n    \n    def print_stats(self):\n        \"\"\"Print parsing statistics.\"\"\"\n        success_rate = 100 * (1 - self.parse_failures / max(1, self.total_calls))\n        print(f\"\\nVLM Statistics:\")\n        print(f\"  Total calls: {self.total_calls}\")\n        print(f\"  Parse failures: {self.parse_failures}\")\n        print(f\"  Success rate: {success_rate:.1f}%\")\n\n# Initialize\nvlm = VLMPredictor()",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized VLM: qwen3-vl:8b\n",
      "Embedding model: qwen3-embedding:latest\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Perturbation Functions"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-11T14:47:23.335359Z",
     "start_time": "2026-01-11T14:47:23.325925Z"
    }
   },
   "source": [
    "def apply_perturbation(image: Image.Image, brightness=0, contrast=1.0, blur=0, noise=0) -> Image.Image:\n",
    "    \"\"\"\n",
    "    Apply perturbations to image.\n",
    "    \n",
    "    Args:\n",
    "        brightness: -0.3 to 0.3 (additive)\n",
    "        contrast: 0.7 to 1.3 (multiplicative)\n",
    "        blur: 0 to 2 (sigma)\n",
    "        noise: 0 to 0.1 (sigma)\n",
    "    \"\"\"\n",
    "    img = image.copy()\n",
    "    \n",
    "    # Brightness\n",
    "    if brightness != 0:\n",
    "        enhancer = ImageEnhance.Brightness(img)\n",
    "        img = enhancer.enhance(1.0 + brightness)\n",
    "    \n",
    "    # Contrast\n",
    "    if contrast != 1.0:\n",
    "        enhancer = ImageEnhance.Contrast(img)\n",
    "        img = enhancer.enhance(contrast)\n",
    "    \n",
    "    # Blur\n",
    "    if blur > 0:\n",
    "        img = img.filter(ImageFilter.GaussianBlur(radius=blur*2))\n",
    "    \n",
    "    # Noise\n",
    "    if noise > 0:\n",
    "        img_array = np.array(img).astype(np.float32)\n",
    "        noise_array = np.random.normal(0, noise*255, img_array.shape)\n",
    "        img_array = np.clip(img_array + noise_array, 0, 255).astype(np.uint8)\n",
    "        img = Image.fromarray(img_array)\n",
    "    \n",
    "    return img\n",
    "\n",
    "def perturbation_magnitude(brightness=0, contrast=1.0, blur=0, noise=0) -> float:\n",
    "    \"\"\"Compute L2 norm of perturbation parameters.\"\"\"\n",
    "    return np.sqrt(brightness**2 + (contrast-1)**2 + blur**2 + noise**2)\n",
    "\n",
    "# Define perturbation grid (5\u00d75 = 25 perturbations)\n",
    "perturbation_grid = [\n",
    "    {'brightness': b, 'contrast': c, 'blur': 0, 'noise': 0}\n",
    "    for b in [-0.2, -0.1, 0, 0.1, 0.2]\n",
    "    for c in [0.8, 0.9, 1.0, 1.1, 1.2]\n",
    "]\n",
    "\n",
    "print(f\"Perturbation grid: {len(perturbation_grid)} configurations\")\n",
    "print(f\"Example: {perturbation_grid[0]}\")\n",
    "print(f\"Magnitude range: [{min(perturbation_magnitude(**p) for p in perturbation_grid):.3f}, {max(perturbation_magnitude(**p) for p in perturbation_grid):.3f}]\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perturbation grid: 25 configurations\n",
      "Example: {'brightness': -0.2, 'contrast': 0.8, 'blur': 0, 'noise': 0}\n",
      "Magnitude range: [0.000, 0.283]\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. IoU Computation"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-11T14:47:23.350771Z",
     "start_time": "2026-01-11T14:47:23.344237Z"
    }
   },
   "source": [
    "def compute_iou(bbox1: np.ndarray, bbox2: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Compute IoU between two normalized bboxes [x1, y1, x2, y2].\n",
    "    \"\"\"\n",
    "    x1_inter = max(bbox1[0], bbox2[0])\n",
    "    y1_inter = max(bbox1[1], bbox2[1])\n",
    "    x2_inter = min(bbox1[2], bbox2[2])\n",
    "    y2_inter = min(bbox1[3], bbox2[3])\n",
    "    \n",
    "    intersection = max(0, x2_inter - x1_inter) * max(0, y2_inter - y1_inter)\n",
    "    \n",
    "    area1 = (bbox1[2] - bbox1[0]) * (bbox1[3] - bbox1[1])\n",
    "    area2 = (bbox2[2] - bbox2[0]) * (bbox2[3] - bbox2[1])\n",
    "    union = area1 + area2 - intersection\n",
    "    \n",
    "    return intersection / union if union > 0 else 0"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run Perturbation Experiment\n",
    "\n",
    "**WARNING:** This cell will make ~1,275 API calls to Ollama (51 samples \u00d7 25 perturbations).\n",
    "\n",
    "Expected runtime: ~60 minutes with Qwen3-VL 8b (~3 sec/inference)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2026-01-11T14:47:23.351366Z"
    }
   },
   "source": "# Storage for results\nresults = {\n    'iou_original': [],\n    'iou_perturbed': [],  # (n_samples, n_perturbations)\n    'perturbation_magnitude': [],\n    'embedding_distance': [],\n    'sample_indices': small_indices.tolist()\n}\n\nprint(f\"Starting perturbation experiment...\")\nprint(f\"Samples: {len(samples)}\")\nprint(f\"Perturbations per sample: {len(perturbation_grid)}\")\ntotal_inferences = len(samples) * (1 + len(perturbation_grid))\nprint(f\"Total inferences: {total_inferences}\")\nprint()\n\nstart_time = time.time()\n\n# Single progress bar for all inferences\npbar = tqdm(total=total_inferences, desc=\"Processing\", unit=\"inference\")\n\nfor sample_idx, sample in enumerate(samples):\n    info = get_sample_info(sample)\n    image = info['image']\n    expression = info['expressions'][0]  # Use first expression\n    bbox_gt = info['bbox_normalized']\n    \n    # Original prediction\n    bbox_pred_orig = vlm.predict_bbox(image, expression)\n    iou_orig = compute_iou(bbox_pred_orig, bbox_gt)\n    embed_orig = vlm.get_embedding(expression)\n    \n    results['iou_original'].append(iou_orig)\n    \n    pbar.update(1)\n    pbar.set_postfix({\n        'Sample': f'{sample_idx+1}/{len(samples)}',\n        'IoU_orig': f'{iou_orig:.3f}'\n    })\n    \n    # Perturbed predictions\n    ious_pert = []\n    mags = []\n    embed_dists = []\n    \n    for pert_idx, pert_config in enumerate(perturbation_grid):\n        # Apply perturbation\n        img_pert = apply_perturbation(image, **pert_config)\n        \n        # Predict\n        bbox_pred_pert = vlm.predict_bbox(img_pert, expression)\n        iou_pert = compute_iou(bbox_pred_pert, bbox_gt)\n        \n        # Get embedding\n        embed_pert = vlm.get_embedding(expression)\n        embed_dist = np.linalg.norm(embed_orig - embed_pert)\n        \n        ious_pert.append(iou_pert)\n        mags.append(perturbation_magnitude(**pert_config))\n        embed_dists.append(embed_dist)\n        \n        pbar.update(1)\n        \n        # Update postfix every 5 perturbations to reduce overhead\n        if (pert_idx + 1) % 5 == 0 or (pert_idx + 1) == len(perturbation_grid):\n            elapsed = time.time() - start_time\n            pbar.set_postfix({\n                'Sample': f'{sample_idx+1}/{len(samples)}',\n                'Pert': f'{pert_idx+1}/{len(perturbation_grid)}',\n                'IoU': f'{iou_pert:.3f}'\n            })\n    \n    results['iou_perturbed'].append(ious_pert)\n    results['perturbation_magnitude'].append(mags)\n    results['embedding_distance'].append(embed_dists)\n\npbar.close()\n\n# Convert to numpy\nfor key in ['iou_original', 'iou_perturbed', 'perturbation_magnitude', 'embedding_distance']:\n    results[key] = np.array(results[key])\n\nelapsed_total = time.time() - start_time\n\nprint(f\"\\n\u2713 Experiment complete!\")\nprint(f\"  Total time: {elapsed_total/60:.1f} minutes\")\nprint(f\"  Average per sample: {elapsed_total/len(samples):.1f} seconds\")\nprint(f\"  Mean IoU (original): {results['iou_original'].mean():.3f}\")\nprint(f\"  Mean IoU (perturbed): {results['iou_perturbed'].mean():.3f}\")\n\n# Print VLM parsing statistics\nvlm.print_stats()\n\n# Save results\nnp.savez('data/threshold_optimization_results.npz', **results)\nprint(f\"\\nSaved results to threshold_optimization_results.npz\")",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting perturbation experiment...\n",
      "Samples: 51\n",
      "Perturbations per sample: 25\n",
      "Total inferences: 1326\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Processing:   0%|          | 0/1326 [00:00<?, ?inference/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cd37dce0413249cb965031d26f4b2bdf"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error predicting bbox: Ollama API call timed out\n",
      "Error predicting bbox: Ollama API call timed out\n",
      "Warning: Invalid bbox [0.002 0.1   0.    0.285] from: '```json\n",
      "[\n",
      "\t{\"bbox_2d\": [100, 0, 285, 266], \"label\": \"top left water\"}\n",
      "]\n",
      "```'\n",
      "Error predicting bbox: Ollama API call timed out\n",
      "Error predicting bbox: Ollama API call timed out\n",
      "Error predicting bbox: Ollama API call timed out\n",
      "Error predicting bbox: Ollama API call timed out\n",
      "Error predicting bbox: Ollama API call timed out\n",
      "Error predicting bbox: Ollama API call timed out\n",
      "Error predicting bbox: Ollama API call timed out\n",
      "Error predicting bbox: Ollama API call timed out\n",
      "Error predicting bbox: Ollama API call timed out\n",
      "Error predicting bbox: Ollama API call timed out\n",
      "Error predicting bbox: Ollama API call timed out\n",
      "Error predicting bbox: Ollama API call timed out\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Data Quality Check"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Load results (if restarting from saved file)\n",
    "# results = dict(np.load('data/threshold_optimization_results.npz'))\n",
    "\n",
    "print(\"Dataset Quality Check:\")\n",
    "print(f\"  Samples: {len(results['iou_original'])}\")\n",
    "print(f\"  Perturbations per sample: {results['iou_perturbed'].shape[1]}\")\n",
    "print(f\"\\nIoU Statistics:\")\n",
    "print(f\"  Original - mean: {results['iou_original'].mean():.3f}, std: {results['iou_original'].std():.3f}\")\n",
    "print(f\"  Perturbed - mean: {results['iou_perturbed'].mean():.3f}, std: {results['iou_perturbed'].std():.3f}\")\n",
    "print(f\"\\nIoU Degradation:\")\n",
    "iou_drop = results['iou_original'][:, np.newaxis] - results['iou_perturbed']\n",
    "print(f\"  Mean drop: {iou_drop.mean():.3f}\")\n",
    "print(f\"  Std drop: {iou_drop.std():.3f}\")\n",
    "print(f\"  Max drop: {iou_drop.max():.3f}\")\n",
    "\n",
    "# Check for sensitivity\n",
    "iou_std = results['iou_perturbed'].std(axis=1).mean()\n",
    "print(f\"\\nPerturbation Sensitivity:\")\n",
    "print(f\"  Mean IoU std across perturbations: {iou_std:.3f}\")\n",
    "if iou_std > 0.1:\n",
    "    print(f\"  \u2713 Model shows perturbation sensitivity\")\n",
    "else:\n",
    "    print(f\"  \u26a0 Model may be too robust (low sensitivity)\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Threshold Optimization Functions"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def iou_to_class(iou: np.ndarray, thresholds: list) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Discretize IoU values into classes.\n",
    "    \n",
    "    Args:\n",
    "        iou: IoU values (any shape)\n",
    "        thresholds: Sorted list [t1, t2, ...] creating len+1 classes\n",
    "    \n",
    "    Returns:\n",
    "        Integer class labels (0, 1, 2, ...)\n",
    "    \"\"\"\n",
    "    classes = np.zeros_like(iou, dtype=int)\n",
    "    for i, t in enumerate(sorted(thresholds)):\n",
    "        classes[iou >= t] = i + 1\n",
    "    return classes\n",
    "\n",
    "def compute_mutual_information(classes: np.ndarray, signal: np.ndarray, n_bins=20) -> float:\n",
    "    \"\"\"\n",
    "    MI between discrete classes and continuous signal.\n",
    "    \n",
    "    Signal is binned into n_bins buckets.\n",
    "    \"\"\"\n",
    "    classes_flat = classes.flatten()\n",
    "    signal_flat = signal.flatten()\n",
    "    \n",
    "    # Bin signal\n",
    "    signal_binned = np.digitize(signal_flat, np.linspace(signal_flat.min(), signal_flat.max(), n_bins))\n",
    "    \n",
    "    return mutual_info_score(classes_flat, signal_binned)\n",
    "\n",
    "def optimize_thresholds(iou: np.ndarray, signal: np.ndarray, n_thresholds=3, n_restarts=10):\n",
    "    \"\"\"\n",
    "    Find thresholds maximizing MI via multi-start L-BFGS-B.\n",
    "    \n",
    "    Returns:\n",
    "        optimal_thresholds, best_mi\n",
    "    \"\"\"\n",
    "    def objective(thresholds):\n",
    "        thresholds = np.sort(np.clip(thresholds, 0.01, 0.99))\n",
    "        classes = iou_to_class(iou, thresholds.tolist())\n",
    "        mi = compute_mutual_information(classes, signal)\n",
    "        return -mi  # Minimize negative MI\n",
    "    \n",
    "    best_result = None\n",
    "    best_mi = -np.inf\n",
    "    \n",
    "    for _ in range(n_restarts):\n",
    "        x0 = np.sort(np.random.uniform(0.1, 0.9, n_thresholds))\n",
    "        bounds = [(0.05, 0.95)] * n_thresholds\n",
    "        \n",
    "        result = optimize.minimize(\n",
    "            objective,\n",
    "            x0,\n",
    "            method='L-BFGS-B',\n",
    "            bounds=bounds\n",
    "        )\n",
    "        \n",
    "        if -result.fun > best_mi:\n",
    "            best_mi = -result.fun\n",
    "            best_result = result\n",
    "    \n",
    "    return np.sort(best_result.x), best_mi"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Optimize Thresholds for Different Signals"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Define signals\n",
    "signals = {\n",
    "    'perturbation_magnitude': results['perturbation_magnitude'],\n",
    "    'embedding_distance': results['embedding_distance'],\n",
    "    'iou_drop': results['iou_original'][:, np.newaxis] - results['iou_perturbed']\n",
    "}\n",
    "\n",
    "# Optimize for each signal\n",
    "optimization_results = {}\n",
    "\n",
    "print(\"Optimizing thresholds for different signals...\\n\")\n",
    "\n",
    "for signal_name, signal in signals.items():\n",
    "    print(f\"Signal: {signal_name}\")\n",
    "    \n",
    "    # Optimize\n",
    "    optimal_thresh, optimal_mi = optimize_thresholds(\n",
    "        results['iou_perturbed'],\n",
    "        signal,\n",
    "        n_thresholds=3,\n",
    "        n_restarts=10\n",
    "    )\n",
    "    \n",
    "    # Baseline (uniform thresholds)\n",
    "    baseline_thresh = [0.3, 0.5, 0.7]\n",
    "    baseline_classes = iou_to_class(results['iou_perturbed'], baseline_thresh)\n",
    "    baseline_mi = compute_mutual_information(baseline_classes, signal)\n",
    "    \n",
    "    improvement = ((optimal_mi - baseline_mi) / baseline_mi) * 100\n",
    "    \n",
    "    optimization_results[signal_name] = {\n",
    "        'optimal_thresholds': optimal_thresh,\n",
    "        'optimal_mi': optimal_mi,\n",
    "        'baseline_mi': baseline_mi,\n",
    "        'improvement_pct': improvement\n",
    "    }\n",
    "    \n",
    "    print(f\"  Optimal thresholds: {optimal_thresh.round(3)}\")\n",
    "    print(f\"  Optimal MI: {optimal_mi:.4f}\")\n",
    "    print(f\"  Baseline MI: {baseline_mi:.4f}\")\n",
    "    print(f\"  Improvement: {improvement:+.1f}%\")\n",
    "    print()\n",
    "\n",
    "# Save optimization results\n",
    "np.savez('data/optimization_results.npz', **optimization_results)\n",
    "print(\"Saved optimization results to optimization_results.npz\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Threshold comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Threshold positions\n",
    "ax = axes[0]\n",
    "colors = plt.cm.Set2.colors\n",
    "\n",
    "for i, (name, res) in enumerate(optimization_results.items()):\n",
    "    thresholds = res['optimal_thresholds']\n",
    "    y_pos = i\n",
    "    \n",
    "    for t in thresholds:\n",
    "        ax.axvline(t, ymin=y_pos/len(optimization_results), \n",
    "                   ymax=(y_pos+0.8)/len(optimization_results), \n",
    "                   color=colors[i], linewidth=3)\n",
    "    \n",
    "    ax.text(0.02, y_pos + 0.4, \n",
    "            f\"{name}\\nMI={res['optimal_mi']:.3f} (+{res['improvement_pct']:.1f}%)\",\n",
    "            transform=ax.get_yaxis_transform(), fontsize=9)\n",
    "\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_ylim(-0.5, len(optimization_results))\n",
    "ax.set_xlabel('IoU Threshold')\n",
    "ax.set_yticks([])\n",
    "ax.set_title('Optimal Thresholds by Signal Type')\n",
    "\n",
    "# Plot 2: MI comparison\n",
    "ax = axes[1]\n",
    "signal_names = list(optimization_results.keys())\n",
    "optimal_mis = [optimization_results[s]['optimal_mi'] for s in signal_names]\n",
    "baseline_mis = [optimization_results[s]['baseline_mi'] for s in signal_names]\n",
    "\n",
    "x = np.arange(len(signal_names))\n",
    "width = 0.35\n",
    "\n",
    "ax.bar(x - width/2, baseline_mis, width, label='Baseline [0.3, 0.5, 0.7]', alpha=0.7)\n",
    "ax.bar(x + width/2, optimal_mis, width, label='Optimized', alpha=0.7)\n",
    "\n",
    "ax.set_ylabel('Mutual Information')\n",
    "ax.set_title('MI: Baseline vs Optimized Thresholds')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([s.replace('_', '\\n') for s in signal_names], fontsize=8)\n",
    "ax.legend()\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Boundary Detection"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Use optimal thresholds from perturbation magnitude signal\n",
    "optimal_thresh = optimization_results['perturbation_magnitude']['optimal_thresholds']\n",
    "\n",
    "# Classify samples\n",
    "classes_orig = iou_to_class(results['iou_original'], optimal_thresh)\n",
    "classes_pert = iou_to_class(results['iou_perturbed'], optimal_thresh)\n",
    "\n",
    "# Detect boundaries: samples where ANY perturbation changes class\n",
    "class_changes = classes_pert != classes_orig[:, np.newaxis]\n",
    "boundary_mask = class_changes.any(axis=1)\n",
    "\n",
    "# Severity: max class jump\n",
    "severity = np.abs(classes_pert - classes_orig[:, np.newaxis]).max(axis=1)\n",
    "\n",
    "print(f\"Boundary Detection Results:\")\n",
    "print(f\"  Optimal thresholds: {optimal_thresh.round(3)}\")\n",
    "print(f\"  Boundary samples: {boundary_mask.sum()} / {len(boundary_mask)} ({100*boundary_mask.mean():.1f}%)\")\n",
    "print(f\"  Severity distribution: {np.bincount(severity)}\")\n",
    "print(f\"\\nClass Distribution:\")\n",
    "print(f\"  Original: {np.bincount(classes_orig)}\")\n",
    "print(f\"  Perturbed: {np.bincount(classes_pert.flatten())}\")\n",
    "\n",
    "# Sanity check\n",
    "if boundary_mask.mean() < 0.2 or boundary_mask.mean() > 0.6:\n",
    "    print(f\"\\n\u26a0 Warning: Boundary rate {100*boundary_mask.mean():.1f}% outside expected range (20-60%)\")\n",
    "else:\n",
    "    print(f\"\\n\u2713 Boundary rate within realistic range\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Findings:\n",
    "1. **MI Improvement:** Check if optimized thresholds beat baseline by >5%\n",
    "2. **Signal Differences:** Do geometric and semantic signals yield different thresholds?\n",
    "3. **Boundary Rate:** Is it realistic (20-40%)?\n",
    "\n",
    "### Next Steps:\n",
    "- If results look good \u2192 Scale to validation set (99 samples) in notebook 03\n",
    "- If MI improvement <5% \u2192 Check data quality, model sensitivity\n",
    "- Apply learned thresholds to new samples for boundary detection"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}